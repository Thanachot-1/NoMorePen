import os
import glob
import re
import torch
import torch.nn as nn
from torchvision import utils
import matplotlib.pyplot as plt

from data.data_collection_and_cleaning import get_data_loader

# ---- Settings ----
batch_size = 32
lr_g = 2e-4
lr_d = 1e-4
latent_dim = 100
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
target_epochs = 1000
gen_updates = 20
output_dir = './output'
os.makedirs(output_dir, exist_ok=True)

# ---- Models ----
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, 256, 7, 1, 0),
            nn.BatchNorm2d(256), nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.BatchNorm2d(128), nn.ReLU(True),
            nn.ConvTranspose2d(128, 1, 4, 2, 1),
            nn.Tanh()
        )
    def forward(self, z): return self.net(z)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 128, 4, 2, 1), nn.LeakyReLU(0.2, True), nn.Dropout(0.3),
            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, True), nn.Dropout(0.3),
            nn.Conv2d(256, 1, 7, 1, 0), nn.Sigmoid()
        )
    def forward(self, x): return self.net(x).view(-1,1)

# ---- Init ----
loader = get_data_loader()
G, D = Generator().to(device), Discriminator().to(device)
criterion = nn.BCELoss()
opt_g = torch.optim.Adam(G.parameters(), lr=lr_g)
opt_d = torch.optim.Adam(D.parameters(), lr=lr_d)

# ---- Checkpoint Resume ----
def last_epoch():
    ckpts = sorted(glob.glob(os.path.join(output_dir, 'generator_epoch_*.pth')),
                   key=lambda p: int(re.search(r'_(\d+)\.pth', p).group(1)))
    if not ckpts: return 0
    return int(re.search(r'_(\d+)\.pth', ckpts[-1]).group(1))

start_epoch = last_epoch()
if start_epoch:
    G.load_state_dict(torch.load(f'{output_dir}/generator_epoch_{start_epoch:03d}.pth', map_location=device))
    D.load_state_dict(torch.load(f'{output_dir}/discriminator_epoch_{start_epoch:03d}.pth', map_location=device))
    print(f"Resumed at epoch {start_epoch}")
end_epoch = start_epoch + target_epochs

# ---- Training Loop & Validation/Error Logging ----
for epoch in range(start_epoch+1, end_epoch+1):
    d_loss_total, g_loss = 0, 0
    for real, _ in loader:
        real = real.to(device)
        b_size = real.size(0)
        # Discriminator step
        opt_d.zero_grad()
        real_labels = torch.ones(b_size,1,device=device)
        fake_labels = torch.zeros(b_size,1,device=device)
        loss_real = criterion(D(real), real_labels)
        fake = G(torch.randn(b_size,latent_dim,1,1,device=device))
        loss_fake = criterion(D(fake.detach()), fake_labels)
        (loss_real + loss_fake).backward(); opt_d.step()
        d_loss_total += (loss_real+loss_fake).item()
        # Generator steps
        for _ in range(gen_updates):
            opt_g.zero_grad()
            gen_imgs = G(torch.randn(b_size,latent_dim,1,1,device=device))
            loss_g = criterion(D(gen_imgs), real_labels)
            loss_g.backward(); opt_g.step()
        g_loss = loss_g.item()
    # Logging
    avg_d_loss = d_loss_total / len(loader)
    print(f"[{epoch}/{end_epoch}] D_loss: {avg_d_loss:.4f} G_loss: {g_loss:.4f}")
    # Save & sample every 10 epochs
    if epoch % 10 == 0 or epoch==end_epoch:
        torch.save(G.state_dict(), f'{output_dir}/generator_epoch_{epoch:03d}.pth')
        torch.save(D.state_dict(), f'{output_dir}/discriminator_epoch_{epoch:03d}.pth')
        with torch.no_grad():
            sample = G(torch.randn(64,latent_dim,1,1,device=device)).cpu()
            grid = utils.make_grid(sample, nrow=8, normalize=True, padding=2)
            plt.figure(figsize=(4,4)); plt.axis('off')
            plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')
            plt.show(); plt.close()
